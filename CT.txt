Together
52860000.0
53720000.0
And maybe we'll come back.
59690000.0
62170000.0
I guess there is no.
67930000.0
69580000.0
It's the final down.
81890000.0
83960000.0
With that
88830000.0
90330000.0
Let's talk about some review.
91000000.0
92780000.0
So.
94310000.0
95210000.0
This is going to go fairly quick.
95960000.0
97550000.0
It took about 1/2 hour last lecture and
97550000.0
100360000.0
I think that's about all there is to
100360000.0
101609000.0
say.
101610000.0
102190000.0
So.
103960000.0
104760000.0
We will start with.
106020000.0
107550000.0
So.
113720000.0
114320000.0
We covered everything up until single
116250000.0
119140000.0
source shortest path or the shortest
119140000.0
121130000.0
path algorithm.
121130000.0
121860000.0
So I've got today to talk about
121860000.0
124436000.0
shortest path algorithms on flow and
124436000.0
129070000.0
finally bloom filters.
129070000.0
130280000.0
The things that you have not been
130280000.0
131580000.0
tested on.
131580000.0
132440000.0
Everything else we've already talked
132440000.0
133596000.0
about as much as I'm going to though at
133596000.0
135615000.0
the end I'll be happy to stay and
135615000.0
137030000.0
answer any questions people have.
137030000.0
138520000.0
So first Dykstra is right.
139390000.0
141090000.0
It's a wonderful algorithm.
141090000.0
142960000.0
And it's fairly easy to understand.
143840000.0
145630000.0
First of all, it was a very simple
145630000.0
147450000.0
translation from PRIMS, but
147450000.0
150120000.0
fundamentally Dijkstra's algorithm
150120000.0
151930000.0
seeks to find the shortest path from
151930000.0
155280000.0
one source to all other points on the
155280000.0
157910000.0
graph, which sounds like a lot, but it
157910000.0
160610000.0
does this very efficiently and it
160610000.0
163320000.0
doesn't efficiently because it assumes
163320000.0
165750000.0
that it is looked at all paths up to a
165750000.0
168700000.0
certain distance.
168700000.0
170320000.0
So it starts out when you pick the
170320000.0
172803000.0
source that has looked at all paths.
172803000.0
174870000.0
Of length zero and everything else is
174940000.0
177410000.0
Infinity.
177410000.0
178080000.0
And it then says, OK, we're going to
178840000.0
181430000.0
look at when we process that zero path.
181430000.0
184270000.0
We're going to look at all its
184270000.0
185850000.0
neighbors and find the shortest path,
185850000.0
188860000.0
including these new edges, right?
188860000.0
190950000.0
And so here we see it finds the 10 and
191840000.0
194985000.0
12 distance.
194985000.0
195970000.0
And then it says we're going to expand
195970000.0
198410000.0
the 10.
198410000.0
199850000.0
Right, because that's the next shortest
200870000.0
202840000.0
path.
202840000.0
203320000.0
And then we're going to expand the 12,
204550000.0
206690000.0
because the distance here was 18 here,
206690000.0
210010000.0
we're not showing the computing along
210010000.0
213175000.0
the way.
213175000.0
213720000.0
We're showing only the final points,
213720000.0
217370000.0
but it's very efficient.
217370000.0
218514000.0
What's wrong with it?
218514000.0
220732000.0
What's wrong with it is in its default
220732000.0
224840000.0
state.
224840000.0
225715000.0
It does not handle.
225715000.0
227920000.0
Negative weights and when we talk about
231260000.0
234036000.0
Dijkstra's in this class and in general
234036000.0
236670000.0
we mean dykstra's in its traditional
236670000.0
239540000.0
form.
239540000.0
239980000.0
So.
241910000.0
242810000.0
After I realized this won't work that
243580000.0
245750000.0
way, they can't write on these slides
245750000.0
248130000.0
anyways.
248130000.0
248850000.0
So what does Dykstra in its traditional
250800000.0
252770000.0
form look like?
252770000.0
253825000.0
Here it is right?
253825000.0
254910000.0
And what we've changed this little bit
254910000.0
256570000.0
down here.
256570000.0
257370000.0
To change it from prims?
258300000.0
259780000.0
And the little bit we changed was
261130000.0
262900000.0
instead of looking at only the lightest
262900000.0
266260000.0
edge and seeing if that is shorter.
266260000.0
269080000.0
We look at the edge plus the distance
270080000.0
272720000.0
to the start of that edge, right?
272720000.0
274640000.0
And so that is a total distance and if
274640000.0
279230000.0
it is shorter we update the total
279230000.0
281960000.0
distance and.
281960000.0
282990000.0
Its predecessor
283750000.0
284700000.0
With that, we're finding the shortest
287010000.0
288810000.0
path, and as I said, the key idea here
288810000.0
291560000.0
is that.
291560000.0
292560000.0
Paths that we're finding can only get
293380000.0
297170000.0
longer, right?
297170000.0
298140000.0
That we started looking at short paths
299160000.0
301303000.0
and since we are adding paths as they
301303000.0
304800000.0
get longer and we're adding the
304800000.0
308130000.0
shortest path we can find that goes to
308130000.0
311200000.0
a new place.
311200000.0
312070000.0
But that is longer than any other path
313390000.0
315360000.0
we've seen, or at least as long.
315360000.0
317280000.0
They can only get longer, and for that
318420000.0
320715000.0
to be true we cannot have negative
320715000.0
322640000.0
edges because then this shortest path
322640000.0
325210000.0
to somewhere could get shorter right?
325210000.0
327470000.0
Whereas to get anywhere we first have
327470000.0
329545000.0
to walk to the edge of our world and
329545000.0
331733000.0
then walk more.
331733000.0
333030000.0
It can only get longer.
333030000.0
334410000.0
OK.
335770000.0
336300000.0
The other thing is to remember this is
338210000.0
340290000.0
how we have always written the
340290000.0
341490000.0
pseudocode.
341490000.0
342440000.0
But these lines hiding something, isn't
343190000.0
345400000.0
it right?
345400000.0
346420000.0
On line 20, it looks like we're
346420000.0
348060000.0
updating an array.
348060000.0
349100000.0
But we're updating a value that causes
350280000.0
352490000.0
our priority queue to be updated, and
352490000.0
355760000.0
so that's possibly taking a lot of
355760000.0
357420000.0
work.
357420000.0
357930000.0
If we use a traditional priority queue
358830000.0
362890000.0
with updates, this is taking log N work
362890000.0
365970000.0
right.
365970000.0
366340000.0
But if we use a Fibonacci heap, which
367430000.0
370990000.0
we discussed but didn't implement.
370990000.0
373960000.0
It's only taking constant time because
374890000.0
377490000.0
the Fibonacci heap has one special
377490000.0
379440000.0
value, which I didn't remind you of
379440000.0
382270000.0
when talking about.
382270000.0
383540000.0
Or spanning trees, but it has the
384640000.0
386520000.0
special feature that.
386520000.0
387940000.0
It has a function that will decrease
388560000.0
390480000.0
key.
390480000.0
390870000.0
That is to say increase the priority of
390870000.0
393990000.0
a key.
393990000.0
394590000.0
And they can do that change in constant
395390000.0
398010000.0
time.
398010000.0
398460000.0
Making the key be lower priority.
399360000.0
401325000.0
That takes work, but making the key be
401325000.0
404380000.0
higher priority.
404380000.0
405286000.0
That is to say, have a smaller value.
405286000.0
408070000.0
We can do that efficiently and with
408070000.0
410770000.0
Dykstra.
410770000.0
411140000.0
The only changes that ever happened is
411140000.0
413790000.0
that the nodes that we have not yet
413790000.0
416360000.0
visited we find shorter and shorter
416360000.0
419159000.0
paths for right.
419160000.0
420270000.0
They all started it Infinity.
420270000.0
422370000.0
And the only time they change is if we
423200000.0
426520000.0
find something shorter.
426520000.0
427610000.0
OK.
428790000.0
429210000.0
So.
431170000.0
432120000.0
That line 20 hides possibly a lot of
434960000.0
438130000.0
work.
438130000.0
438590000.0
But it could be managed by using the
439260000.0
442680000.0
correct data structure.
442680000.0
443970000.0
I could go through running through the
446510000.0
448310000.0
algorithm, but I think that seeing that
448310000.0
452134000.0
the one time was good enough right, we
452134000.0
454940000.0
keep going through and adding the
454940000.0
456553000.0
smallest node and adding the smallest
456553000.0
458119000.0
node.
458119000.0
458379000.0
And I will put a link to all these
460030000.0
461890000.0
slides up so you've got it.
461890000.0
463470000.0
So we end up with a couple of things to
466870000.0
469160000.0
review.
469160000.0
469530000.0
Ken Dykstra's handle.
469530000.0
472170000.0
An undirected graph.
473520000.0
474637000.0
We described it with a directed graph.
474637000.0
477310000.0
But yeah, we can easily handle an
477310000.0
479427000.0
undirected graph.
479427000.0
480282000.0
We just treat it as if there is an edge
480282000.0
483580000.0
in both directions, right?
483580000.0
485310000.0
So we update both ways, not a problem.
485310000.0
487570000.0
Can it handle a negative cycle well?
491440000.0
495300000.0
Can any shortest path handle a negative
496010000.0
498390000.0
cycle?
498390000.0
498760000.0
No, because it doesn't make sense
499520000.0
501210000.0
right?
501210000.0
502080000.0
If there's a negative cycle in my
502080000.0
503890000.0
graph.
503890000.0
504580000.0
The shortest path to any place that can
505980000.0
508910000.0
reach that negative cycle is go to the
508910000.0
512241000.0
negative cycle.
512241000.0
512850000.0
Do that as many times as you want and
513600000.0
516340000.0
then go to your destination.
516340000.0
517650000.0
And you can always be shorter by going
518480000.0
521680000.0
through the cycle one more time, so
521680000.0
524220000.0
there is no shortest path, right?
524220000.0
526710000.0
So that's important to remember that as
527460000.0
529140000.0
shortest path is not defined.
529140000.0
532130000.0
If we have a negative weight cycle, can
532130000.0
534805000.0
dykstra's do negative weight edges?
534805000.0
537390000.0
I've already said the default digesters
537390000.0
540510000.0
cannot, but we did discuss a small
540510000.0
543850000.0
change right in dykstra's.
543850000.0
546143000.0
We know we will look at each vertex.
546143000.0
549370000.0
We will add each vertex as the shortest
550140000.0
552860000.0
element once right, every time we go
552860000.0
556250000.0
through our loop, we are adding one
556250000.0
557890000.0
vertex.
557890000.0
559240000.0
We'll do that for every vertex.
559240000.0
560880000.0
When we're done, we will add it all the
560880000.0
563120000.0
vertices and we found the shortest path
563120000.0
565090000.0
to everywhere.
565090000.0
565640000.0
But we can simply change Dijkstra's in
566420000.0
568810000.0
a very small way.
568810000.0
570220000.0
We can say when we add a vertex that's
570970000.0
574760000.0
the shortest vertex, we know a path to
574760000.0
577790000.0
great.
577790000.0
578320000.0
We process it just and we update any
578320000.0
583315000.0
vertex, even if we have already seen
583315000.0
585720000.0
it.
585720000.0
586040000.0
With a path that's shorter.
586910000.0
589470000.0
If we do that, we run Intel.
590970000.0
593550000.0
There are no vertices to visit until
593550000.0
596300000.0
our priority queue is empty.
596300000.0
597920000.0
And then we're done.
598700000.0
599750000.0
Cool.
601040000.0
601690000.0
So that's a very small change, but it
602940000.0
604870000.0
changes our analysis right in the sick
604870000.0
608570000.0
dykers the one that we talked about,
608570000.0
610760000.0
the one that Dykstra defined you have
610760000.0
613630000.0
this feature that you will go through
613630000.0
616055000.0
that loop exactly as many times as
616055000.0
618810000.0
their vertices, because every time
618810000.0
620720000.0
through the loop.
620720000.0
621479000.0
You add 1 vertex and you will never
622280000.0
625620000.0
revisit it.
625620000.0
626430000.0
With modified Dijkstra's, you have no
627160000.0
629880000.0
such guarantee and we did not do the
629880000.0
632800000.0
analysis of modified Dijkstra's because
632800000.0
635220000.0
it isn't really important because
635220000.0
637820000.0
fundamentally there are no better ways
637820000.0
643080000.0
but in general.
643080000.0
644490000.0
This negative weight edges there are
646080000.0
647400000.0
lots of different things we can apply.
647400000.0
649190000.0
OK, so that's what I've got to say
650110000.0
652320000.0
about digesters.
652320000.0
653180000.0
Yeah I should.
655710000.0
656440000.0
Running times, this is the one we talk
657850000.0
660010000.0
about N log N + M.
660010000.0
662200000.0
That's the one that is known that
662200000.0
665100000.0
requires a Fibonacci heap or a similar
665100000.0
667780000.0
heap that has constant time reduce
667780000.0
673190000.0
increased priority.
673190000.0
674440000.0
If we do it with our basic data
676500000.0
678200000.0
structure, we get M log N.
678200000.0
680590000.0
OK.
682610000.0
683030000.0
Last thing to say in this that is not
684460000.0
687550000.0
likely to really show up on the final
687550000.0
689270000.0
but wasn't big impact for people in the
689270000.0
692020000.0
projects is.
692020000.0
693560000.0
If you're doing dykstra's, you have a
694610000.0
696380000.0
real problem.
696380000.0
697470000.0
Because the standard priority queues
698130000.0
700480000.0
you're given.
700480000.0
701220000.0
Don't actually have update, so the
702150000.0
704350000.0
standard template library has a heap
704350000.0
706490000.0
priority queue.
706490000.0
707790000.0
That heap is the very one we
707790000.0
709410000.0
implemented right where we use an
709410000.0
711360000.0
array.
711360000.0
711920000.0
What's the problem with that?
713090000.0
714960000.0
The problem.
716980000.0
717920000.0
Is that with an array?
718890000.0
719775000.0
I don't know where my vertex is right?
719775000.0
722820000.0
It's somewhere in the array, so to
722820000.0
726230000.0
update it I would have to potentially
726230000.0
728240000.0
search the whole array.
728240000.0
729380000.0
We kind of tricked you because in the
730580000.0
732440000.0
lab we had you do updates right?
732440000.0
734470000.0
But that's because we controlled
734470000.0
736370000.0
exactly.
736370000.0
736845000.0
We knew where things would be.
736845000.0
738760000.0
And we just wanted you to see the idea,
739790000.0
741900000.0
O, if you could figure out the vertex
741900000.0
743900000.0
you want to update, you can do updates
743900000.0
746720000.0
of priority in log N time either by
746720000.0
749600000.0
heapify or heapify down.
749600000.0
751320000.0
But the trick is, how do you find the
752520000.0
754510000.0
vertex?
754510000.0
755150000.0
Well, if this is a feature you want to
755150000.0
757720000.0
support in general, you then implement
757720000.0
760270000.0
a priority queue using pointers instead
760270000.0
763270000.0
of using the array solution.
763270000.0
764860000.0
It's very straightforward change, and
764860000.0
767687000.0
then since the memory location storing
767687000.0
771970000.0
any value in the priority queue doesn't
771970000.0
774310000.0
change, just the pointers to its
774310000.0
777760000.0
parents and children.
777760000.0
779250000.0
It's fine.
780350000.0
781030000.0
Everything works the same, the
781680000.0
783000000.0
algorithms the same, we just have to
783000000.0
784640000.0
play with pointers instead of array
784640000.0
786780000.0
indexing.
786780000.0
787360000.0
OK.
788560000.0
788950000.0
But that wasn't it for shortest paths.
790750000.0
793630000.0
We also talked about an algorithm.
793630000.0
795510000.0
Floyd warshall.
795510000.0
797010000.0
I love this thing because you can write
797010000.0
800545000.0
this code in about 10 minutes if you
800545000.0
802920000.0
already have a graph, right?
802920000.0
804680000.0
If someone else has provided you the
804680000.0
806500000.0
graph implementation implementing Floyd
806500000.0
809166000.0
Warshall.
809166000.0
809819000.0
This practically drops in and becomes
811300000.0
814050000.0
C.
814050000.0
814715000.0
You could probably just copy this in.
814715000.0
817200000.0
Have the compiler scream at you and
817200000.0
818880000.0
figure out what the debug right when I
818880000.0
821979000.0
was much younger.
821980000.0
822930000.0
Some people I knew took when there were
822930000.0
826460000.0
lots of language programming classes.
826460000.0
828393000.0
They took Fortran, COBOL, basic and see
828393000.0
832540000.0
all at the same time they had the same
832540000.0
834700000.0
assignments.
834700000.0
835410000.0
They wrote them in one language and
835410000.0
837170000.0
then just debugged them by dumping them
837170000.0
838840000.0
into the others.
838840000.0
839730000.0
We don't recommend this in general, but
841150000.0
843790000.0
straightforward.
843790000.0
844505000.0
It's not that hard to convert.
844505000.0
847910000.0
So why is it interesting?
848680000.0
851060000.0
First of all.
851750000.0
853860000.0
Floyd Warshall
854530000.0
855720000.0
Handily automatically by itself,
856420000.0
858690000.0
handles negative weight edges, that's
858690000.0
860190000.0
nice.
860190000.0
860650000.0
But it's also interesting in that.
861560000.0
863810000.0
It's the first in this class concrete
864690000.0
866880000.0
example of dynamic programming.
866880000.0
869090000.0
It's a dynamic programming algorithm.
870940000.0
872780000.0
What do I mean by that?
872780000.0
873800000.0
I mean, it builds in from on
873800000.0
875870000.0
information of previous iterations, so
875870000.0
878750000.0
the first time we go through this loop
878750000.0
881940000.0
right, we initialize the world and we
881940000.0
885510000.0
know the shortest path from anywhere to
885510000.0
889110000.0
anywhere that consists only of the
889110000.0
892699000.0
start and the end.
892700000.0
893850000.0
Right?
894740000.0
895130000.0
That's exactly the cost of the edges.
895970000.0
898960000.0
We're done.
900480000.0
901070000.0
We've we've solved that sub problem.
901070000.0
904620000.0
We go on with Floyd Warshall.
906170000.0
908480000.0
And.
909440000.0
910190000.0
We now look through.
911160000.0
914090000.0
The path that includes.
915600000.0
919030000.0
The first value in our implementation.
920020000.0
923010000.0
In this case a.
924170000.0
925730000.0
So it includes the beginning of the end
925730000.0
928720000.0
in A and we've now solved that sub
928720000.0
932210000.0
problem.
932210000.0
932850000.0
And we know those values.
934130000.0
935560000.0
Those may be the shortest path, they
935560000.0
937500000.0
may not.
937500000.0
938140000.0
It isn't clear how it's being dynamic
939680000.0
942520000.0
programming yet, how it's building on
942520000.0
943980000.0
sub problems.
943980000.0
944890000.0
But then.
946420000.0
947310000.0
We look at the next one, and here we're
949530000.0
953210000.0
writing it as if it is the start, the
953210000.0
956110000.0
end and B.
956110000.0
957310000.0
But in fact it's solving all possible
958230000.0
962220000.0
paths that are the start the end and A
962220000.0
966034000.0
or B or A&amp;B in between.
966034000.0
969480000.0
Right, because it computes, what if I
970400000.0
975680000.0
included be in the path?
975680000.0
977230000.0
Is it better?
977230000.0
978170000.0
If so, let's do it.
978970000.0
981450000.0
Otherwise, we'll stick with something
981450000.0
983680000.0
we had.
983680000.0
984446000.0
We already had the shortest path of.
984446000.0
987140000.0
Nothing of nothing in a now we say,
987970000.0
992600000.0
well, what about B?
992600000.0
994690000.0
But this B implicitly includes a path
994690000.0
999309000.0
to B involving A and implicitly
999310000.0
1003317000.0
includes a path from B involving a.
1003317000.0
1007719000.0
So we now have found that and we could
1009160000.0
1013290000.0
think of this triply nested loop as a
1013290000.0
1015430000.0
huge recursion.
1015430000.0
1016680000.0
Right where we would say recursively we
1017370000.0
1020450000.0
want to find the shortest path.
1020450000.0
1022090000.0
Our recursion would look for shortest
1022090000.0
1024170000.0
paths missing one thing and go down.
1024170000.0
1026960000.0
The base of that recursion we would
1027750000.0
1029462000.0
have the shortest path involving
1029462000.0
1031570000.0
nothing but the start and the end, and
1031570000.0
1033660000.0
then one up we would have involving a
1033660000.0
1036720000.0
involving B involving C involving D
1036720000.0
1040620000.0
right?
1040620000.0
1041130000.0
All of those are already computed, and
1042120000.0
1044455000.0
we build up and we build up and we
1044455000.0
1047388000.0
build up.
1047388000.0
1048039000.0
And the trick is we don't need extra
1048970000.0
1050700000.0
space because we only care about
1050700000.0
1053540000.0
keeping track of the shortest one.
1053540000.0
1055360000.0
Right, we don't need all of the
1056080000.0
1057540000.0
possible sub problems.
1057540000.0
1059130000.0
We only need the shortest sub problem
1059130000.0
1061850000.0
solution we found between the start and
1061850000.0
1064625000.0
the end.
1064625000.0
1065050000.0
So I think that's really cool.
1067200000.0
1069990000.0
It's a nice algorithm.
1069990000.0
1072290000.0
Why do we like it?
1072290000.0
1073690000.0
If we compare it to dykstra's.
1074640000.0
1077830000.0
It's iffy what I like man, it's so much
1079340000.0
1082850000.0
easier to code than diestrus.
1082850000.0
1084290000.0
And your time matters.
1085240000.0
1087070000.0
If I really need all the points.
1089030000.0
1091480000.0
Is it better than dykstra's?
1093750000.0
1095330000.0
Not quite.
1095330000.0
1096340000.0
By graph has a lot of edges right?
1097130000.0
1100200000.0
Because Dijkstra's has this term here.
1100200000.0
1103250000.0
That is.
1104370000.0
1105260000.0
There's about the edges, the M term,
1106830000.0
1109460000.0
and if that term is large, as in the
1109460000.0
1112530000.0
number of edges are on the order of N
1112530000.0
1115570000.0
squared, then.
1115570000.0
1117570000.0
Oh
1118490000.0
1118870000.0
That's north squared.
1119820000.0
1120950000.0
And in a concrete actual application,
1122410000.0
1125700000.0
it has another disadvantage.
1125700000.0
1127660000.0
Arrays are nice compact data
1128530000.0
1130510000.0
structures, and going through an array
1130510000.0
1132660000.0
in a nice, predictable manner is good
1132660000.0
1136080000.0
for cash performance.
1136080000.0
1137810000.0
Dykstra is just it's touching
1137810000.0
1140040000.0
everything.
1140040000.0
1140335000.0
It's hopping around.
1140335000.0
1141233000.0
It's going.
1141233000.0
1141779000.0
Hey, I'm only going to do the work I
1141780000.0
1143540000.0
need to do, and that's great.
1143540000.0
1146120000.0
Except for if the computer is already
1147030000.0
1149360000.0
getting all of the work, you don't need
1149360000.0
1151190000.0
to do.
1151190000.0
1151720000.0
It doesn't really help you as much.
1152490000.0
1154280000.0
So cash is make Floyd Warshall happy
1155880000.0
1160200000.0
and dykstra's sad.
1160200000.0
1162370000.0
And finally of course.
1162370000.0
1164550000.0
Floyd Warshall automatically handles
1165180000.0
1167480000.0
negative edges if we redo dykstra's
1167480000.0
1171570000.0
with negative edges are problem becomes
1171570000.0
1174820000.0
that loop bound is possibly worse.
1174820000.0
1179010000.0
And everything gets bad because the
1179720000.0
1182480000.0
loop bound of M ends up over here.
1182480000.0
1185390000.0
That every edge could potentially
1186570000.0
1188740000.0
update the thing and could return the
1188740000.0
1191810000.0
vertex.
1191810000.0
1192560000.0
So OK.
1193240000.0
1195610000.0
So if we need negative weight edges,
1197060000.0
1199050000.0
it's useful, and if we need.
1199050000.0
1201170000.0
You deal with large things with a lot
1202380000.0
1206100000.0
of edges, it's useful.
1206100000.0
1207260000.0
OK, that's what we did for shortest
1208360000.0
1210480000.0
path.
1210480000.0
1211010000.0
Neither are terribly hard, both are, I
1212330000.0
1215490000.0
think, fairly easy to understand.
1215490000.0
1217380000.0
Floyd Warshall I think is trivial to
1218090000.0
1220170000.0
understand the code.
1220170000.0
1221430000.0
Really, wrapping your head around the
1222220000.0
1223870000.0
concept is a little bit more
1223870000.0
1225330000.0
challenging.
1225330000.0
1225980000.0
That's OK.
1227240000.0
1227850000.0
So then we talked about network flow.
1229290000.0
1231180000.0
Right and we said OK, look.
1232280000.0
1234760000.0
It's spent one day sort of working
1234760000.0
1236775000.0
through the algorithm and then a bunch
1236775000.0
1238770000.0
of time working through the proof.
1238770000.0
1240460000.0
And the answer is you don't have to
1241080000.0
1242940000.0
remember the proof, so that's nice from
1242940000.0
1245300000.0
a test point of view, but I thought it
1245300000.0
1247220000.0
was kind of interesting and getting
1247220000.0
1249650000.0
comfortable with the ideas and the
1249650000.0
1251183000.0
proof will help you in 374.
1251183000.0
1253010000.0
Not because they going to do the same
1253810000.0
1255070000.0
thing, but they're going to do similar
1255070000.0
1256400000.0
kinds of ideas so.
1256400000.0
1259200000.0
Flow problem of Max flow is trying to
1260570000.0
1263030000.0
solve what is the flow from 1 vertex in
1263030000.0
1267610000.0
a graph that's traditionally directed
1267610000.0
1269910000.0
to another?
1269910000.0
1270900000.0
OK, we call that a flow network.
1271960000.0
1275470000.0
The first thing we do is we need to
1276340000.0
1278460000.0
talk about what is the flow on a path.
1278460000.0
1281250000.0
Here we have the path AB, CF.
1282040000.0
1285150000.0
What's the flow?
1285970000.0
1286900000.0
Let's do we can figure out the flow of
1289060000.0
1291080000.0
a BCF.
1291080000.0
1291960000.0
It's 3, right?
1294040000.0
1295450000.0
Because the flow, the most material or
1296210000.0
1300130000.0
water or whatever I can put through
1300130000.0
1302460000.0
those connections is equal to the
1302460000.0
1304990000.0
smallest choke point, right?
1304990000.0
1307190000.0
If I try to put four in, sure it goes.
1307190000.0
1309710000.0
Here it goes here and then it goes.
1309710000.0
1311019000.0
And get stopped.
1312480000.0
1313390000.0
So we get three and DEF.
1314520000.0
1317002000.0
We get def.
1317002000.0
1319060000.0
We get 7 or I think that was actually
1319810000.0
1323570000.0
supposed to be a DF.
1323570000.0
1326520000.0
No, it was.
1328670000.0
1329660000.0
To get five, it must have been
1334430000.0
1335990000.0
replacing CF with DEF.
1335990000.0
1339930000.0
Forgive me.
1339930000.0
1340890000.0
I'm using slides that have been done by
1340890000.0
1342905000.0
TA's for review in past semesters, so
1342905000.0
1346210000.0
again, we can compute that.
1346210000.0
1348870000.0
Pretty cool.
1350060000.0
1351050000.0
And then how did we compute the flow?
1352720000.0
1354453000.0
We kept picking paths until we there
1354453000.0
1357840000.0
were no more paths, right?
1357840000.0
1359430000.0
We computed the flow and this works
1359430000.0
1362390000.0
great and we could look at these things
1362390000.0
1364190000.0
and say let's see this one, it was.
1364190000.0
1367770000.0
I think we got.
1374220000.0
1375350000.0
Look
1378640000.0
1379230000.0
We got 10.
1380670000.0
1381860000.0
And we got 20.
1382720000.0
1383770000.0
I did it right in my head.
1384930000.0
1386090000.0
OK.
1387540000.0
1388230000.0
And our choke points end up being CF.
1389890000.0
1393550000.0
And DE.
1394200000.0
1395230000.0
Over here, our choke points are well
1396370000.0
1399230000.0
either AB and AC.
1399230000.0
1401420000.0
Or BD and CD.
1402340000.0
1405330000.0
But there was a problem here, right
1407500000.0
1410300000.0
when we looked at this second graph, it
1410300000.0
1413300000.0
could make bad choices, right?
1413300000.0
1414990000.0
That I could choose.
1416550000.0
1417820000.0
Hey, I'm gonna pick a random flow and
1417820000.0
1419830000.0
the first flow I'm gonna pick is ABCD.
1419830000.0
1423540000.0
Right, and so I flow 5 units.
1424490000.0
1427600000.0
But I've consumed 5 of this.
1428430000.0
1431450000.0
And that means I can only because the
1432470000.0
1435356000.0
path AC has no other exit to get to D
1435356000.0
1440640000.0
the path AC can only flow 5 and that's
1440640000.0
1444800000.0
reduced the total we can flow out of
1444800000.0
1446755000.0
the origin.
1446755000.0
1447320000.0
Right, so how did we fix it?
1448630000.0
1450670000.0
Well?
1450670000.0
1451240000.0
The first thing we did was we said,
1452000000.0
1454060000.0
hey.
1454060000.0
1454540000.0
Let's do something fancier.
1455630000.0
1457320000.0
Instead of removing flows from the
1457320000.0
1459500000.0
graph when we remove a flow, we're
1459500000.0
1463120000.0
going to add a backwards flow, right?
1463120000.0
1465230000.0
Which is a way of letting the algorithm
1465230000.0
1467980000.0
kind of undo its mistake.
1467980000.0
1469640000.0
We said if we have a backwards flow, we
1470340000.0
1473340000.0
can just each keep adding flows that
1473340000.0
1476336000.0
will happen is that backwards flow will
1476336000.0
1478960000.0
get undone O when we flowed from A to
1478960000.0
1481897000.0
B&amp;B to C&amp;C to D we said now.
1481897000.0
1484640000.0
The Edge BC is all used up, but there's
1485390000.0
1489420000.0
an edge.
1489420000.0
1490160000.0
CB with a capacity of five.
1490950000.0
1494810000.0
And that edge can be used to undo the
1496160000.0
1499460000.0
mistake.
1499460000.0
1500100000.0
OK.
1501370000.0
1501790000.0
So that's cool.
1503260000.0
1504260000.0
But it takes time.
1505040000.0
1506205000.0
We showed that as long as we have
1506205000.0
1507999000.0
integers, it ends because every time we
1508000000.0
1511300000.0
run a flow we have increased the total
1511300000.0
1513830000.0
flow.
1513830000.0
1514250000.0
And with integers after a certain
1515400000.0
1517370000.0
number of integers, we'll get to any
1517370000.0
1518910000.0
fixed integer.
1518910000.0
1519850000.0
And since flow is going to be a fixed
1519850000.0
1521960000.0
value, that was cool.
1521960000.0
1523510000.0
But then we said, hey.
1524860000.0
1526190000.0
We could do something better.
1527100000.0
1528590000.0
We could be smarter.
1528590000.0
1530210000.0
About how we pick our edges right?
1531120000.0
1533050000.0
And there were two ways we could be
1534320000.0
1535890000.0
smarter.
1535890000.0
1536450000.0
We could pick edges that were the
1537560000.0
1540300000.0
largest path, right?
1540300000.0
1542110000.0
So if we flowed, the most possible for
1542110000.0
1544810000.0
example here, we could pick a BD or a
1544810000.0
1548820000.0
CD, but we could not pick a BCD.
1548820000.0
1552290000.0
Both of them, because both of those two
1553170000.0
1554950000.0
are larger.
1554950000.0
1555550000.0
They're 10 versus 5.
1555550000.0
1557110000.0
Then it would work and we wouldn't have
1557820000.0
1559210000.0
to worry about the backwards.
1559210000.0
1560430000.0
We still need the backwards because
1561360000.0
1562930000.0
there's much more complicated graphs
1562930000.0
1565600000.0
you'll still need the backwards, but
1565600000.0
1566830000.0
it's more efficient.
1566830000.0
1567750000.0
We said, OK, that's one, but it's a
1568610000.0
1571530000.0
little complicated to figure out the
1571530000.0
1573240000.0
largest path, right?
1573240000.0
1574860000.0
We said another thing we could do is
1574860000.0
1577720000.0
just choose the shortest path and
1577720000.0
1579750000.0
number of edges, and again that would
1579750000.0
1582260000.0
allow Abd and a CD.
1582260000.0
1585740000.0
And then that was what our whole proof
1586680000.0
1588585000.0
was showing.
1588585000.0
1589650000.0
That's really good.
1589650000.0
1591565000.0
That is an algorithm that guarantees
1591565000.0
1594650000.0
things converge very quickly.
1594650000.0
1596330000.0
We don't care about the details, but
1597290000.0
1599270000.0
understanding that those two choices as
1599270000.0
1601790000.0
heuristics instead of picking a
1601790000.0
1603880000.0
completely random path, greatly improve
1603880000.0
1606645000.0
the algorithm.
1606645000.0
1607500000.0
Things that would not picking the path
1607500000.0
1610290000.0
with the largest single edge.
1610290000.0
1614060000.0
It's not necessarily good, right?
1615200000.0
1616690000.0
The path ABC D has an edge of 10.
1617750000.0
1622420000.0
Picking a path with the longest path,
1623760000.0
1628430000.0
clearly bad, et cetera.
1628430000.0
1630740000.0
OK.
1631580000.0
1631960000.0
So that's really all we had to do on
1632890000.0
1634410000.0
flow.
1634410000.0
1635000000.0
That's the only amount I want you to
1635000000.0
1636910000.0
understand is sort of how to run the
1636910000.0
1638530000.0
basic algorithm.
1638530000.0
1640150000.0
Why back edges are there, how they're
1640150000.0
1643340000.0
used to fix things, and what are good
1643340000.0
1646000000.0
heuristics for path choice.
1646000000.0
1647460000.0
That's the only things we could ask
1648230000.0
1649530000.0
about on their final.
1649530000.0
1651350000.0
And finally, Brad talked about Bloom
1653870000.0
1656348000.0
filters.
1656348000.0
1656782000.0
Bloom filters are super cool and it's
1656782000.0
1659730000.0
the first time we've covered them.
1659730000.0
1660980000.0
Bloom filters really want to use very
1662850000.0
1666110000.0
little space to store a huge amount of
1666110000.0
1668520000.0
data.
1668520000.0
1668940000.0
And they also want to be very fast.
1669740000.0
1671550000.0
And so their idea is kind of well.
1672230000.0
1674420000.0
Let's have a hash table.
1675220000.0
1676530000.0
But instead of storing all those values
1677470000.0
1679860000.0
and things, we're just going to store.
1679860000.0
1682037000.0
Is there something here or not, right?
1682037000.0
1684860000.0
So what we're storing?
1684860000.0
1686530000.0
Either there's something here or there
1686530000.0
1688871000.0
is not, so it's true or false.
1688871000.0
1691320000.0
And when we insert we go to every hash
1692100000.0
1696640000.0
value.
1696640000.0
1697250000.0
We take our data and we hash it and we
1698570000.0
1701069000.0
set it to one.
1701070000.0
1701840000.0
If it's zero, it goes to one.
1702620000.0
1704153000.0
If it's one, it goes to 1, right?
1704153000.0
1705750000.0
We cannot remove from a basic bloom
1707770000.0
1709890000.0
filter because we have no way of
1709890000.0
1712470000.0
knowing if that being is that hash
1712470000.0
1715200000.0
value is set to 1 because we inserted
1715200000.0
1719490000.0
the thing that we want to remove or we
1719490000.0
1721475000.0
inserted something else.
1721475000.0
1722600000.0
OK.
1723790000.0
1724400000.0
So one of the promise when we look at a
1726770000.0
1731790000.0
data structure answering a question, a
1731790000.0
1734722000.0
data structure has four possible
1734722000.0
1736680000.0
outcomes to an answer of a question,
1736680000.0
1738450000.0
right?
1738450000.0
1738940000.0
It can say yes, this thing is true.
1738940000.0
1741700000.0
Here's the answer.
1741700000.0
1743369000.0
No, it is false.
1743370000.0
1746290000.0
And we are correct.
1747020000.0
1748189000.0
It could accidentally say.
1749220000.0
1750940000.0
Sure, I've got that information and be
1750940000.0
1753290000.0
completely wrong, or it could say.
1753290000.0
1756670000.0
That doesn't exist and be completely
1757390000.0
1760000000.0
wrong, right?
1760000000.0
1760810000.0
These are the four things we can have
1760810000.0
1762680000.0
happen.
1762680000.0
1763100000.0
If our data structure is not 100%
1763780000.0
1766400000.0
correct, which is the case of bloom
1766400000.0
1768720000.0
filters, right?
1768720000.0
1769410000.0
They can be wrong.
1769410000.0
1770630000.0
But one of these bloom filters cannot.
1771770000.0
1774900000.0
What is it that cannot be true from a
1776110000.0
1778220000.0
bloom filter?
1778220000.0
1778850000.0
Yeah.
1778850000.0
1779320000.0
A bloom filter cannot have a false
1781540000.0
1784090000.0
negative that if I've inserted
1784090000.0
1786880000.0
something.
1786880000.0
1787570000.0
It is definitely got a one on it right?
1788600000.0
1791600000.0
If I have not inserted it.
1791600000.0
1794160000.0
It might have a 0, but something else
1794950000.0
1798560000.0
might have hashed to that value.
1798560000.0
1800730000.0
So we might say, yeah, that's here.
1800730000.0
1802320000.0
But we cannot have it not be here.
1803870000.0
1806110000.0
So with that information we said, can
1807550000.0
1810085000.0
we make bloom filters a little bit more
1810085000.0
1811920000.0
accurate?
1811920000.0
1812430000.0
And instead of using 1 hash value.
1813380000.0
1815940000.0
We're going to use multiple hash
1817150000.0
1819380000.0
values, right?
1819380000.0
1820301000.0
We're going to hash the thing that we
1820301000.0
1823080000.0
want to put in, or check in the bloom
1823080000.0
1825380000.0
filter with several different hash
1825380000.0
1827980000.0
functions.
1827980000.0
1828640000.0
And will check all those locations on
1829260000.0
1832340000.0
insert we will set the mall to one and
1832340000.0
1835357000.0
on search checking if there we will
1835357000.0
1838400000.0
look at all of them.
1838400000.0
1839906000.0
If any of them are zero the answer is
1839906000.0
1842060000.0
no.
1842060000.0
1842410000.0
It's not here, but if they're all one,
1842410000.0
1844870000.0
the answer is yes and this improves.
1844870000.0
1848580000.0
Our performance in general and it's
1849360000.0
1853290000.0
basically banking on this idea that it
1853290000.0
1855960000.0
can't have a false negative.
1855960000.0
1859630000.0
So.
1861480000.0
1863930000.0
This kind of comp incomprehensible
1865110000.0
1867840000.0
function, which you don't have to
1867840000.0
1869460000.0
memorize, describes the chances.
1869460000.0
1872640000.0
Assuming sue haha.
1873760000.0
1875260000.0
But what's interesting about it?
1875260000.0
1877050000.0
What's interesting, right?
1877950000.0
1879230000.0
We have the.
1879230000.0
1881470000.0
M is the size of our vector that we're
1882600000.0
1885830000.0
storing our bloom filter in.
1885830000.0
1887300000.0
K is the number of hash functions.
1888090000.0
1890770000.0
And N is the number of things we
1891530000.0
1894180000.0
insert, right?
1894180000.0
1895140000.0
And what's interesting here is OK, both
1896250000.0
1899450000.0
makes this bigger and smaller.
1899450000.0
1902560000.0
So as I use more and more hash
1903680000.0
1907450000.0
functions, I get more and more accurate
1907450000.0
1910050000.0
until kind of I'm filling in the whole
1910050000.0
1912947000.0
table, right?
1912947000.0
1913730000.0
If I have 100 hash functions and a
1913730000.0
1916848000.0
table with only ten locations, I'm just
1916848000.0
1920190000.0
going to color the whole table as true.
1920190000.0
1922350000.0
The first thing I insert.
1922350000.0
1924050000.0
And now I am almost always having false
1924990000.0
1927830000.0
negatives or false positives, right?
1927830000.0
1930310000.0
Because everything's gonna say it's
1930310000.0
1931590000.0
there no problem.
1931590000.0
1932320000.0
But if I only have 1 hash function.
1933610000.0
1935600000.0
I can end up with the chance of having
1937750000.0
1941910000.0
a false.
1941910000.0
1942700000.0
A positive is exactly one over the size
1943490000.0
1946500000.0
of the table.
1946500000.0
1947240000.0
Right, because I assume my hash
1947980000.0
1949970000.0
functions are suha.
1949970000.0
1951040000.0
So there's this curve we get.
1952100000.0
1953770000.0
And what does this mean?
1954730000.0
1955860000.0
This means that we reduce that
1955860000.0
1959750000.0
complicated mass to this optimal point.
1959750000.0
1963340000.0
You don't actually have to memorize
1964160000.0
1965530000.0
this function either, but you should
1965530000.0
1966960000.0
remember sort of what's going on with
1966960000.0
1968620000.0
it that this optimal point says that
1968620000.0
1973830000.0
for a given number of keys and a given
1973830000.0
1976509000.0
size of.
1976510000.0
1978040000.0
Table we have a optimal number of
1978920000.0
1982264000.0
insertions for an expected number of
1982264000.0
1985131000.0
insertions expected in a given size of
1985131000.0
1988223000.0
table.
1988223000.0
1988537000.0
We have an optimal number of hash
1988537000.0
1990710000.0
functions for a given, number, of,
1990710000.0
1993539000.0
etcetera.
1993540000.0
1994020000.0
So we can solve for any two of these.
1994020000.0
1996870000.0
We can figure out the best choice of
1996870000.0
1998576000.0
the third cool.
1998576000.0
2000540000.0
When designing your hash function your
2002070000.0
2004830000.0
bloom filters you want to base on this
2004830000.0
2007250000.0
OK.
2007250000.0
2007700000.0
Finally, we talked about one
2010300000.0
2012259000.0
improvement over a classical bloom
2012259000.0
2014455000.0
filter, which was a counting bloom
2014455000.0
2016480000.0
filter.
2016480000.0
2016850000.0
And the counting bloom filter.
2018020000.0
2019375000.0
We want to be able to ask how many
2019375000.0
2021400000.0
different things did we see or how many
2021400000.0
2024430000.0
things of this type did we see, right?
2024430000.0
2026270000.0
Of this set of things, and this gives
2027360000.0
2031510000.0
us some features.
2031510000.0
2032565000.0
This says that instead of storing true
2032565000.0
2034980000.0
or false, we'll store a count, right.
2034980000.0
2038590000.0
This makes it use more space.
2038590000.0
2040870000.0
But
2042150000.0
2042980000.0
It makes it mean that we have an idea
2044510000.0
2047580000.0
of how many different things we saw.
2047580000.0
2049360000.0
OK, that seems useful.
2050610000.0
2052500000.0
It also allows us to kind of remove,
2054490000.0
2058100000.0
right?
2058100000.0
2059470000.0
It's not perfect.
2059470000.0
2060840000.0
But we now have a way of removing
2061560000.0
2063880000.0
because all we're going to do is
2063880000.0
2065250000.0
decrement the count if we have put two
2065250000.0
2067720000.0
things in that collided.
2067720000.0
2069140000.0
And we decrement and we remove one of
2069800000.0
2071610000.0
them.
2071610000.0
2071896000.0
We decrement the count by 1, and that's
2071896000.0
2074090000.0
a reasonable approximation.
2074090000.0
2076030000.0
Unfortunately, we then no longer have
2077090000.0
2080840000.0
the promise that we cannot have false
2080840000.0
2083330000.0
negatives.
2083330000.0
2083940000.0
So it's a trade off and it depends on
2084750000.0
2087050000.0
your application design.
2087050000.0
2088600000.0
What do you want to do?
2088600000.0
2089970000.0
There isn't a right answer here, right?
2089970000.0
2092050000.0
We can decide one thing or the other.
2092740000.0
2095110000.0
But it gives us a useful feature.
2096900000.0
2098860000.0
And that's it for what we're going to
2099730000.0
2101430000.0
cover on bloom filters.
2101430000.0
2102690000.0
We're not going to cover on the sketch
2103470000.0
2106200000.0
stuff.
2106200000.0
2106650000.0
We felt that there was a little bit too
2107330000.0
2109060000.0
much probability and we didn't give
2109060000.0
2110490000.0
enough background to test that.
2110490000.0
2112150000.0
But even with it's a cool application
2112950000.0
2115830000.0
that has some really interesting ideas.
2115830000.0
2117740000.0
So as I said, I'll put this link up,
2119080000.0
2121860000.0
but the last thing I wanna play is what
2121860000.0
2124945000.0
this little bit that a.
2124945000.0
2127270000.0
Past TA did that.
2127960000.0
2130810000.0
I think is amazing.
2132369000.0
2134340000.0
He was a grad student when I was still
2139110000.0
2140460000.0
a grad student, so.
2140460000.0
2141320000.0
Heads up, our final is coming soon.
2145560000.0
2148066000.0
1/3 of your entire grade.
2148066000.0
2150950000.0
It's really important.
2150950000.0
2152660000.0
We are here to help you study.
2152660000.0
2155220000.0
So let's go over hashing first, then
2155220000.0
2157990000.0
arrange with the hash function.
2157990000.0
2159810000.0
H deterministic fall for should be all
2159810000.0
2164124000.0
of one.
2164124000.0
2164992000.0
You should remember Sue haha.
2164992000.0
2167300000.0
Uniform distribution.
2169770000.0
2172340000.0
Visions 1-2 keys hash into the same
2179290000.0
2182900000.0
spot.
2182900000.0
2183400000.0
Separate chaining makes all this stuff
2188670000.0
2191175000.0
all your room.
2191175000.0
2192340000.0
You must free hash items may not go to
2194830000.0
2197770000.0
the same place that they were before
2197770000.0
2200610000.0
and that is hashing.
2200610000.0
2203290000.0
Let's move on.
2203290000.0
2204370000.0
Thanks.
2210590000.0
2211170000.0
Heaps the priority queues Intertan move
2214090000.0
2217657000.0
our log in time from route to the
2217657000.0
2220470000.0
leaves is always increasing.
2220470000.0
2223960000.0
There are we stopping our and log in
2223960000.0
2227980000.0
one is over then so make sure these is
2227980000.0
2232220000.0
clear they fest this joint sets also
2232220000.0
2235270000.0
might be.
2235270000.0
2236070000.0
So let's see.
2239890000.0
2240930000.0
OK, start out at minus one and merge if
2242910000.0
2246370000.0
they are not in nothing that so
2246370000.0
2249380000.0
carefully.
2249380000.0
2251200000.0
Union by height.
2253440000.0
2254550000.0
You can be union by size and if you do
2254550000.0
2258240000.0
it right, three efficiency will be
2258240000.0
2260830000.0
maximized.
2260830000.0
2261830000.0
You should use path compression, it
2261830000.0
2263965000.0
gets you up the constant time your tree
2263965000.0
2267740000.0
gets shorter with every single call to
2267740000.0
2270610000.0
find and just like that made amazed
2270610000.0
2274510000.0
figure it out, disjoint sets and all
2274510000.0
2276560000.0
your MP seven days.
2276560000.0
2278800000.0
Therefore, we're moving on to graphs.
2279680000.0
2282740000.0
Questions are about weighted wraps.
2304450000.0
2307540000.0
These kids get their own swimming pool.
2307540000.0
2310620000.0
You can use cross schools and door
2310620000.0
2312890000.0
Prins.
2312890000.0
2313360000.0
It doesn't matter which one.
2313360000.0
2315830000.0
They both create a tree with small this
2315830000.0
2318520000.0
total weight.
2318520000.0
2320070000.0
You'd better and hold the difference
2320820000.0
2322690000.0
between our B&amp;D DFS and which
2322690000.0
2326600000.0
implementation will suit your graph.
2326600000.0
2329173000.0
The best adjacency matrix sore list,
2329173000.0
2333070000.0
which one will make your graph run the
2333070000.0
2335020000.0
fastest without any tricks?
2335020000.0
2337420000.0
Which is n + m plus up if you want the
2338350000.0
2341930000.0
shortest path from S2 N, like through
2341930000.0
2345970000.0
the algorithm is surely the way to go.
2345970000.0
2349700000.0
Need to get this tree is to have all
2351060000.0
2353760000.0
your graph edges being weighted
2353760000.0
2355570000.0
positively.
2355570000.0
2356700000.0
And that's all I've got.
2357730000.0
2361310000.0
Good luck on your exam.
2361860000.0
2363480000.0
